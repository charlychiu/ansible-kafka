---
# The Apache Kafka version to be downloaded and installed
# kafka_download_base_url should be set to https://archive.apache.org/dist/kafka/ for older versions than the current
kafka_download_base_url: https://downloads.apache.org/kafka
kafka_download_validate_certs: yes
kafka_version: 4.0.1
kafka_scala_version: 2.13

# The kafka user and group to create files/dirs with and for running the kafka service
kafka_create_user_group: true
kafka_user: kafka
kafka_group: kafka

kafka_root_dir: /opt
kafka_dir: "{{ kafka_root_dir }}/kafka"

# The application log folder (e.g: server.log)
kafka_log_dir: /var/log/kafka

# Start kafka after installation
kafka_start: yes
# Restart kafka on configuration change
kafka_restart: yes

############################# Server / KRaft #############################

# The node id for this server in KRaft mode. Must be unique within the cluster.
# This replaces broker.id in ZooKeeper mode.
kafka_node_id: 1

# The roles this server will play: 'broker', 'controller', or 'broker,controller'
# Combined mode (broker,controller) is simpler for small clusters
# Separate roles recommended for production clusters with many partitions
kafka_process_roles: "broker,controller"

# Controller quorum voter list: id@host:port,id@host:port,...
# Example for 3-node cluster: "1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093"
# For single-node development: "1@localhost:9093"
# Note: This can be omitted when using controller.quorum.bootstrap.servers
kafka_controller_quorum_voters: "1@localhost:9093"

# Controller quorum bootstrap servers for dynamic quorum (Kafka 4.x+)
# Example: "kafka-1:9093,kafka-2:9093,kafka-3:9093"
# This is used with --initial-controllers for dynamic quorum setup
kafka_controller_quorum_bootstrap_servers: ""

# KRaft storage format mode for kafka-storage.sh format command (Kafka 4.x+)
#
# Two approaches for dynamic quorum setup:
#
# Approach A - Incremental (Recommended):
#   First node:  kafka_storage_format_mode: "standalone"
#   Other nodes: kafka_storage_format_mode: "no-initial-controllers"
#
# Approach B - All-at-once:
#   All initial nodes: kafka_storage_format_mode: "initial-controllers"
#                     (All nodes must use identical kafka_initial_controllers list)
#
# Options:
#   "standalone"            - First node only (Approach A), creates single-voter cluster
#   "initial-controllers"   - All initial nodes (Approach B), requires kafka_initial_controllers
#   "no-initial-controllers" - Nodes joining existing cluster (both approaches)
#   "" (empty)              - Default static quorum (uses controller.quorum.voters)
kafka_storage_format_mode: ""

# Initial controllers list for Approach B (all initial nodes use IDENTICAL value)
# Only used when kafka_storage_format_mode="initial-controllers"
# Format: id@hostname:port:directory_uuid,id@hostname:port:directory_uuid,...
# Example: "1@kafka-1:9093:MvDxzVmcRsaTz33bUuRU6A,2@kafka-2:9093:07R5amHmR32VDA6jHkGbTA,3@kafka-3:9093:JEXY6aqzQY-32P5TStzaFg"
# Note: Directory UUIDs obtained by running: kafka-storage.sh random-uuid
kafka_initial_controllers: ""

# Optional: Pre-defined cluster UUID (if not set, a random UUID will be generated)
# This should be the same across all nodes in the cluster
kafka_cluster_uuid: ""

# The Java heap size (memory) allocation (xmx, xms)
kafka_java_heap: "-Xms1G -Xmx1G"

# Packages and JAVA_HOME used to satisfy Kafka's Java 17+ requirement.
kafka_java_packages: []
kafka_java_home: ""
kafka_java_system_path: "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = security_protocol://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
# listeners=PLAINTEXT://:9092
# KRaft mode requires CONTROLLER listener in addition to broker listeners
# By default, listeners are automatically configured based on kafka_process_roles:
#   - broker role: PLAINTEXT://:9092
#   - controller role: CONTROLLER://:9093 (added automatically if 'controller' in process.roles)
# You can override this by explicitly setting kafka_listeners (e.g., for custom ports or security protocols)
kafka_listeners: []

# Controller listener name (required for KRaft mode)
kafka_controller_listener_names: "CONTROLLER"

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
# advertised.listeners=PLAINTEXT://your.host.name:9092
# By default, advertised listeners are automatically configured based on kafka_process_roles:
#   - broker role: PLAINTEXT://<hostname>:9092
#   - controller role: CONTROLLER://<hostname>:9093 (added automatically if 'controller' in process.roles)
# You can override this by explicitly setting kafka_advertised_listeners
# kafka_advertised_listeners:
#   - "SASL_SSL://:9094"
#   - "PLAINTEXT://:9092"
kafka_advertised_listeners: []

# The number of threads handling network requests
kafka_num_network_threads: 3
# The number of threads that the server uses for processing requests, which may include disk I/O
kafka_num_io_threads: 8
# Specify the number of threads that are used to replicate messages from a source broker. Increasing this value can lead to increased parallelism in I/O operations in the broker.
kafka_num_replica_fetchers: 1

# The send buffer (SO_SNDBUF) used by the socket server
kafka_socket_send_buffer_bytes: 102400
# The receive buffer (SO_RCVBUF) used by the socket server
kafka_socket_receive_buffer_bytes: 102400
# The maximum size of a request that the socket server will accept (protection against OOM)
kafka_socket_request_max_bytes: 104857600
# The socket receive buffer for network requests
kafka_replica_socket_receive_buffer_bytes: 65536

# A comma separated list of directories under which to store data log files
kafka_data_log_dirs: /var/lib/kafka/logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
kafka_num_partitions: 1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
kafka_num_recovery_threads_per_data_dir: 1

# The number of background threads to use for log cleaning
kafka_log_cleaner_threads: 1

# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
kafka_offsets_topic_replication_factor: 3
kafka_transaction_state_log_replication_factor: 3
kafka_transaction_state_log_min_isr: 2

# The minimum age of a log file to be eligible for deletion
kafka_log_retention_hours: 168

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
kafka_log_segment_bytes: 1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
kafka_log_retention_check_interval_ms: 300000

# Enable auto creation of topic on the server
kafka_auto_create_topics_enable: false

# Enables delete topic. Delete topic through the admin tool will have no
# effect if this config is turned off
kafka_delete_topic_enable: true

# Default replication factor for automatically created topics.
kafka_default_replication_factor: 1

############################# Timeout #############################

# Offset commit will be delayed until all replicas for the offsets topic receive the commit or this timeout is reached. This is similar to the producer request timeout.
kafka_offsets_commit_timeout_ms: 5000

# Max wait time for each fetcher request issued by follower replicas. This value should always be less than the replica.lag.time.max.ms at all times to prevent frequent shrinking of ISR for low throughput topics
kafka_replica_fetch_wait_max_ms: 500

# The amount of time to sleep when there are no logs to clean
kafka_log_cleaner_backoff_ms: 15000

########################### Kafka Connect #############################

kafka_connect_bootstrap_servers: "localhost:9092"
kafka_connect_group_id: connect-cluster
kafka_connect_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_key_converter_schemas_enable: true
kafka_connect_value_converter_schemas_enable: true
kafka_connect_internal_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_key_converter_schemas_enable: false
kafka_connect_internal_value_converter_schemas_enable: false
kafka_connect_offset_storage_replication_factor: 1
kafka_connect_config_storage_replication_factor: 1
kafka_connect_status_storage_replication_factor: 1
kafka_connect_offset_flush_interval_ms: 10000
kafka_connect_plugin_path: /usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors

kafka_connect_offset_storage_file_filename: /tmp/connect.offsets

############################# Producer #############################

kafka_producer_bootstrap_servers: "localhost:9092"
kafka_producer_compression_type: none

############################# Consumer #############################

kafka_consumer_group_id: kafka-consumer-group
kafka_consumer_bootstrap_servers: "localhost:9092"

############################# JMX Exporter for Prometheus #############################

# Enable JMX Exporter for Prometheus monitoring
kafka_jmx_exporter_enabled: false

# JMX Exporter version to download
kafka_jmx_exporter_version: 1.5.0

# JMX Exporter download URL
# Note: Versions >= 1.1.0 are downloaded from GitHub Releases (not yet in Maven Central)
#       Versions <= 1.0.1 use Maven Central
kafka_jmx_exporter_url: "https://github.com/prometheus/jmx_exporter/releases/download/{{ kafka_jmx_exporter_version }}/jmx_prometheus_javaagent-{{ kafka_jmx_exporter_version }}.jar"

# JMX Exporter port for Prometheus metrics scraping
kafka_jmx_exporter_port: 7071

# JMX Exporter startup delay in seconds before collecting metrics
# Recommended values:
#   - Development/Testing: 15-30 seconds (faster feedback)
#   - Production: 60-120 seconds (ensure Kafka is fully initialized)
#   - Immediate collection: 0 (may show incomplete metrics during startup)
kafka_jmx_exporter_start_delay_seconds: 30

# JMX Exporter installation directory
kafka_jmx_exporter_dir: "{{ kafka_dir }}/jmx_exporter"

# JMX Exporter jar file path
kafka_jmx_exporter_jar: "{{ kafka_jmx_exporter_dir }}/jmx_prometheus_javaagent-{{ kafka_jmx_exporter_version }}.jar"

# JMX Exporter configuration file path
kafka_jmx_exporter_config: "{{ kafka_jmx_exporter_dir }}/kafka-jmx-exporter.yml"
