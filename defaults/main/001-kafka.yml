---
# The Apache Kafka version to be downloaded and installed
# kafka_download_base_url should be set to https://archive.apache.org/dist/kafka/ for older versions than the current
kafka_download_base_url: https://downloads.apache.org/kafka
kafka_download_validate_certs: yes
kafka_version: 4.0.1
kafka_scala_version: 2.13

# The kafka user and group to create files/dirs with and for running the kafka service
kafka_create_user_group: true
kafka_user: kafka
kafka_group: kafka

kafka_root_dir: /opt
kafka_dir: "{{ kafka_root_dir }}/kafka"

# The application log folder (e.g: server.log)
kafka_log_dir: /var/log/kafka

# Start kafka after installation
kafka_start: yes
# Restart kafka on configuration change
kafka_restart: yes

############################# Server / KRaft #############################

# The node id for this server in KRaft mode. Must be unique within the cluster.
# This replaces broker.id in ZooKeeper mode.
kafka_node_id: 1

# The roles this server will play: 'broker', 'controller', or 'broker,controller'
# Combined mode (broker,controller) is simpler for small clusters
# Separate roles recommended for production clusters with many partitions
kafka_process_roles: "broker,controller"

# Controller quorum voter list: id@host:port,id@host:port,...
# Example for 3-node cluster: "1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093"
# For single-node development: "1@localhost:9093"
# Note: This can be omitted when using controller.quorum.bootstrap.servers
kafka_controller_quorum_voters: "1@localhost:9093"

# Controller quorum bootstrap servers for dynamic quorum (Kafka 4.x+)
# Example: "kafka-1:9093,kafka-2:9093,kafka-3:9093"
# This is used with --initial-controllers for dynamic quorum setup
kafka_controller_quorum_bootstrap_servers: ""

# KRaft storage format mode for kafka-storage.sh format command (Kafka 4.x+)
# Options:
#   "initial-controllers" - Use --initial-controllers flag for dynamic quorum (first node in cluster)
#   "no-initial-controllers" - Use --no-initial-controllers flag (additional nodes joining cluster)
#   "standalone" - Use --standalone flag for single-node dynamic quorum
#   "" (empty) - Use default behavior (static quorum with controller.quorum.voters)
kafka_storage_format_mode: ""

# Initial controllers list for dynamic quorum topology (used when kafka_storage_format_mode="initial-controllers")
# Format: id@hostname:port:directory,id@hostname:port:directory,...
# Example: "1@kafka-1:9093:MvDxzVmcRsaTz33bUuRU6A,2@kafka-2:9093:07R5amHmR32VDA6jHkGbTA,3@kafka-3:9093:JEXY6aqzQY-32P5TStzaFg"
# Note: The directory UUID can be obtained by running kafka-storage.sh random-uuid
# The same values must be used to format all nodes in the initial controller set
kafka_initial_controllers: ""

# Optional: Pre-defined cluster UUID (if not set, a random UUID will be generated)
# This should be the same across all nodes in the cluster
kafka_cluster_uuid: ""

# The Java heap size (memory) allocation (xmx, xms)
kafka_java_heap: "-Xms1G -Xmx1G"

# Packages and JAVA_HOME used to satisfy Kafka's Java 17+ requirement.
kafka_java_packages: []
kafka_java_home: ""

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = security_protocol://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
# listeners=PLAINTEXT://:9092
# KRaft mode requires CONTROLLER listener in addition to broker listeners
kafka_listeners:
  - "PLAINTEXT://:9092"
  - "CONTROLLER://:9093"

# Controller listener name (required for KRaft mode)
kafka_controller_listener_names: "CONTROLLER"

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
# advertised.listeners=PLAINTEXT://your.host.name:9092
# kafka_advertised_listeners:
#   - "SASL_SSL://:9094"
#   - "PLAINTEXT://:9092"

# The number of threads handling network requests
kafka_num_network_threads: 3
# The number of threads that the server uses for processing requests, which may include disk I/O
kafka_num_io_threads: 8
# Specify the number of threads that are used to replicate messages from a source broker. Increasing this value can lead to increased parallelism in I/O operations in the broker.
kafka_num_replica_fetchers: 1

# The send buffer (SO_SNDBUF) used by the socket server
kafka_socket_send_buffer_bytes: 102400
# The receive buffer (SO_RCVBUF) used by the socket server
kafka_socket_receive_buffer_bytes: 102400
# The maximum size of a request that the socket server will accept (protection against OOM)
kafka_socket_request_max_bytes: 104857600
# The socket receive buffer for network requests
kafka_replica_socket_receive_buffer_bytes: 65536

# A comma separated list of directories under which to store data log files
kafka_data_log_dirs: /var/lib/kafka/logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
kafka_num_partitions: 1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
kafka_num_recovery_threads_per_data_dir: 1

# The number of background threads to use for log cleaning
kafka_log_cleaner_threads: 1

# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
kafka_offsets_topic_replication_factor: 1
kafka_transaction_state_log_replication_factor: 1
kafka_transaction_state_log_min_isr: 1

# The minimum age of a log file to be eligible for deletion
kafka_log_retention_hours: 168

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
kafka_log_segment_bytes: 1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
kafka_log_retention_check_interval_ms: 300000

# Enable auto creation of topic on the server
kafka_auto_create_topics_enable: false

# Enables delete topic. Delete topic through the admin tool will have no
# effect if this config is turned off
kafka_delete_topic_enable: true

# Default replication factor for automatically created topics.
kafka_default_replication_factor: 1

kafka_group_initial_rebalance_delay_ms: 0

############################# Timeout #############################

# Offset commit will be delayed until all replicas for the offsets topic receive the commit or this timeout is reached. This is similar to the producer request timeout.
kafka_offsets_commit_timeout_ms: 5000

# Max wait time for each fetcher request issued by follower replicas. This value should always be less than the replica.lag.time.max.ms at all times to prevent frequent shrinking of ISR for low throughput topics
kafka_replica_fetch_wait_max_ms: 500

# The amount of time to sleep when there are no logs to clean
kafka_log_cleaner_backoff_ms: 15000

########################### Kafka Connect #############################

kafka_connect_bootstrap_servers: "localhost:9092"
kafka_connect_group_id: connect-cluster
kafka_connect_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_key_converter_schemas_enable: true
kafka_connect_value_converter_schemas_enable: true
kafka_connect_internal_key_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_value_converter: org.apache.kafka.connect.json.JsonConverter
kafka_connect_internal_key_converter_schemas_enable: false
kafka_connect_internal_value_converter_schemas_enable: false
kafka_connect_offset_storage_replication_factor: 1
kafka_connect_config_storage_replication_factor: 1
kafka_connect_status_storage_replication_factor: 1
kafka_connect_offset_flush_interval_ms: 10000
kafka_connect_plugin_path: /usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors

kafka_connect_offset_storage_file_filename: /tmp/connect.offsets

############################# Producer #############################

kafka_producer_bootstrap_servers: "localhost:9092"
kafka_producer_compression_type: none

############################# Consumer #############################

kafka_consumer_group_id: kafka-consumer-group
kafka_consumer_bootstrap_servers: "localhost:9092"

# Example authentication
############################# AUTHENTICATION #############################
# kafka_listener_auth:
#   - name: "listener.name.sasl_ssl.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule"
#     username: "admin"
#     password: "admin-secret"
#     users:
#       - user: "test1"
#         pass: "test1-secret"
#       - user: "test2"
#         pass: "test2-secret"
#
# kafka_sasl_enabled_mechanisms: "PLAIN"
# kafka_ssl:
#   cipher_suites: "TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384"
#   truststore_location: "/etc/security/broker.keystore.jks"
#   truststore_password: "secret"
#   keystore_location: "/etc/security/broker.keystore.jks"
#   keystore_password: "secret"
#   key_password: "secret"
#   client_auth: "required"

# Here you can define custom opts e.g. settings for JMX Exporter
# kafka_opts: ""
